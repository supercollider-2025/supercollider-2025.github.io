
+++
date = '2025-01-24T21:30:10-05:00'
draft = false
title = 'Concert 4'
+++

Friday, March 14, 2025  
7:30pm  
Location TBA


***Sigil II: Amistad*** by Monte Taylor  
&emsp;&emsp;*Doug O'Connor, saxophone*  



***saccades*** by Ted Moore  
&emsp;&emsp;*Doug O'Connor, saxophone*  



**Improvisation** by Kerrith Livengood  
&emsp;&emsp;*Kerrith Livengood, electronics & instruments*  

---

## Program Notes

### *Sigil II: Amistad*

by [Monte Taylor](/bios/#monte-taylor)

*Sigil II: Amistad* uses Supercollider to generate synthesized soundscapes in realtime, which a live saxophonist must interact with and respond to as they perform the music presented to them on the score. This interaction is at the core of the programmatic elements of the piece, which reflects on notions of cultural diversity and fusion as presented in José Parlá's Mural "Amistad America". Supercollider is also used to apply audio effects to the saxophone's signal to further color the interaction between the performer and computer.

### *saccades*

by [Ted Moore](/bios/#ted-moore)

A “saccade” is a rapid movement of the eyeball between two fixed focal points. During this brief moment, the brain hides this blurry motion from our perception. Once a saccade motion has begun, the destination cannot change, meaning that if the target of focus disappears the viewer won’t know until the saccade completes. If the field of vision is changing too quickly, the saccades may never be able to arrive at and focus on a target, instead, the objects in view are only perceived through peripheral vision. 

This idea is imitated by the sound and video presented in the piece. It also serves as a metaphor for the density of information and high entropy experiences we’re constantly trying to cope with. A scroll on social media, smartphone alerts, big data, technological advancements and predictions, the abundance of choices in the grocery aisle.

The processes used to create *saccades* embody this density of information–and the constant struggle to track it all. Large datasets of audio analyses derived from the tape part are sent to neural networks and dimensionality reduction algorithms to find patterns and then visualize and sonify what is found. A plethora of variations on visual themes are created by the combinatorics of stochastically triggered visual synthesis modules and processing effects. Computer vision analysis adds layers to our visual and aural perception–tightly binding together the visual and auditory elements. In the final movement, source video of an eyeball is morphed via a convolutional neural network creating eyeballs that feel both simple and obvious, but also slightly make my stomach squirm.

### ***Improvisation** by Kerrith Livengood*

by [Kerrith Livengood](/bios/#kerrith-livengood)

I am an improvising performer who is currently developing an ambitious multi-component performance array in SuperCollider. This elaborate setup includes unique live processing effects combining pitch-tracking and looping in complex chaotic ways. The project allows for integration of live instruments, MIDI instruments using VST plugins, and live coding into a single performance by a single performer. I've been performing live improv sets using different versions of this array over the last few months, with many different combinations of synthesizers, instruments, and musical toys. I'd like to present a performance using this multi-component array, as well as possible discussion of collaborative and multimedia possibilities, especially dance and live video improvisation.

---

## Performer Bios

[Doug O'Connor](/bios/#doug-o'connor)  

