{
  "days": [
    {
      "day": "Thursday, March 13, 2025",
      "events": [
        {
          "title": "Coffee",
          "location": "Location TBA",
          "time": "8:30am - 9am",
          "type": "coffee"
        },
        {
          "title": "Keynote Address: James McCartney",
          "location": "Location TBA",
          "speaker": "James McCartney",
          "time": "9am - 10am",
          "type": "keynote"
        },
        {
          "title": "Paper Session 1",
          "location": "Location TBA",
          "time": "10:20am - 11:40am",
          "type": "paper",
          "presenters": [
            {
              "author": "Mike McCormick",
              "time": "10:20am",
              "remote": false,
              "title": "YAWN",
              "abstract": "The experimental metal band YAWN formed in 2020 during the COVID-19 pandemic with the goal of creating and performing uncompromising instrumental music characterized by low-tuned guitars, a complex rhythmic language, and detailed sound design.\n\nHigh production value is a defining feature of this genre, and instead of relying on commercial softwares that impose certain normative performance practices, we decided shortly after the band's formation to rely on SuperCollider to synchronize click track playback, MIDI automation of effects and presets on digital guitar amplifiers, backing track playback, and our pre-programmed light show. After several years of touring with this setup, the software has grown to accommodate unconventional performative or rhythmic concepts and our musical aesthetic has fully embraced the idiosyncrasies afforded by this tailor-made approach.\n\nIn this talk I will present an overview of the SuperCollider classes I've been developing for YAWN since 2020, how the system has evolved to address the unique performative challenges presented by our music, and how it interfaces with the rest of our setup in concert. I'll also share how SuperCollider has been a central tool in our ongoing areas of research, which include generative aural \"scores\" and tempo polyphony."
            },
            {
              "author": "Anna Xambó",
              "remote": true,
              "time": "10:40am",
              "title": "MIRLCa",
              "abstract": "In this presentation, I will introduce the state of affairs of the development of MIRLCa, a self-built tool in SuperCollider. The ongoing SC extension is a user-friendly live coding environment that allows the live coder to query crowdsourced sounds from the Freesound online database using MIR techniques together with interactive machine learning based on the FluCoMa library. This results in a crafted sound-based music style governed by a diverse chorale of sounds that the live coder attempts to tame. The presentation will cover both technical developments and artistic outcomes of using the tool."
            },
            {
              "author": "James Harkins",
              "time": "11am",
              "remote": true,
              "title": "JITModular",
              "abstract": "SuperCollider's Just-In-Time Library (JITLib) supports free experimentation with a wide variety of signal processing strategies, making it a good choice for SynthDef development and for teaching non-programmers. On its own, however, particularly for students, it poses some challenges: a lack of clear best practices for managing signal topology and routing, and the fact that a JITLib code document reflects a work-in-progress and may not unambiguously show the current state of the system. My JITModular project uses JITLib as a monophonic modular synthesizer, building on top of JITLib: 1/ clear, consistent recommendations for signal handling; 2/ a graphical interface to the side of the code window, providing access to parameters and displaying the current state; 3/ unified controls; 4/ a patch-saving mechanism that preserves code, state and resources (e.g. buffers and MIDI controllers). This workshop will introduce these features and demonstrate their pedagogical value. (Topics: live coding, extensions of sclang)"
            },
            {
              "author": "Bruno Gola",
              "time": "11:20am",
              "remote": false,
              "title": "supercollider-gst-rtp",
              "abstract": "In this paper presentation I introduce a framework for multichannel audio streaming from SuperCollider to mobile phones and other devices that support WebRTC, using RTP (Real-time Transport Protocol) and WebRTC together with the Janus WebRTC Gateway. The main feature related to SuperCollider is the supercollider-gst-rtp plugin, published as an opensource SuperCollider plugin. The supercollider-gst-rtp plugin allows the user to define multiple inputs (on different UDP ports) and outputs (as a combination of host and port) to stream any audio signal direct from a Synth definition, or to receive an audio stream also inside the Synth definition. This approach opens possibilities not only for multichannel pieces but also for easy and flexible audio communication over the network direct from SuperCollider without the need of external tools."
            }
          ]
        },
        {
          "title": "Concert 1",
          "location": "Location TBA",
          "time": "1pm",
          "type": "concert",
          "works": [
            {
              "composer": "Luis Alfonso Tamagnini",
              "title": "_hypercubx A/V",
              "duration": 10,
              "performers": [
                "Luis Alfonso Tamagnini, electronics"
              ],
              "sound_check": "11:45am - 12:15am",
              "program-note": "*_hypercubx* exists between a mathematical dimension and the singularity of its volumetric projection . The algorithmic flow models numerical meta-forms into sculptures of sound and light. The system is hacked in performance to break structures and express noise.",
              "tech_needs": "PA & Monitoring Main PA 2.1 (possible multichannel setups up to 8 channels). Stereo monitoring on stage  Screen Flat led screen or proyector. Aspect ratio 16:9. 5 meters width or more. Full HD / 60 fps.  Stage 2 x 1 x 0.8 meters stand. 2 (or more) direct boxes. HDMI output. Video monitor  https://www.unexcoder.com.ar/epk"
            },
            {
              "composer": "Marcin Pietruszewski",
              "title": "the New Pulsar Generator",
              "duration": 10,
              "sound_check": "11:15am - 11:45am",
              "performers": [
                "Marcin Pietruszewski, the New Pulsar Generator"
              ],
              "program-note": "The concert features a selection of compositions conceived with the New Pulsar Generator (nuPG) program for digital sound synthesis designed by the composer Marcin Pietruszewski. The New Pulsar Generator (nuPg) is an interactive program for sound synthesis developed in SuperCollider 3 (SC3) programming language. The nuPg program produces a form of synthesis called pulsar synthesis (PS). The technique of PS operationalises the notion of rhythm with its multitemporal affordances as a system of interconnected patterns evolving on multiple timescales. The technique generates a complex hybrid of sounds across the perceptual time span between infrasonic pulsations and audio frequencies, giving rise to a broad family of musical structures: singular impulses, sequences, continuous tones, time-varying phrases, and beating textures. See: https://www.marcinpietruszewski.com/the-new-pulsar-generator",
              "tech_needs": "- a multichannel sound system  - a table + chair "
            },
            {
              "composer": "Rachel Devorah Rome",
              "title": "RILF",
              "duration": 27,
              "sound_check": "10:45am - 11:15am",
              "performers": [
                "Rachel Devorah Rome, electronics"
              ],
              "program-note": "RILF (the ‘R’ stands for robot…) explores resonances in the (uncanny) valley between feminized machines and machinized femmes. Live coding in SuperCollider and TidalCycles with and alongside bespoke SID chip hardware synthesizers, I resignify signal from “Monica” (the Google AI assistant) answering psychologist Arthur Aron’s “36 questions to fall in love”; “Ann Steel” (composer Roberto Cacciapaglia’s feminized disco avatar); auto-tuned Cher; Donna Summer with an early drum machine; “Samantha” (the Apple text-to-speech voice) iterating on linguistics work by Natural Language Processing (NPL) researcher Bill MacCartney; TikTok streamer “Pinky” (Fedha Sinon) performing as a Non-Playable (videogame) Character (NPC) for the music producer Timbaland; the feminized techno avatar of Uwe Schmidt & Pete Namlook; Zelda; and the actor Scarlett Johansson singing as the character “Samantha” in the 2013 film “her” through Holly Herndon’s “Holly+” AI vocal clone.",
              "tech_needs": "1 HDMI out; 2 XLR out; 1.2m wide x 60cm deep table 1.2m high (I'm 1.7m tall and like to stand..)"
            },
            {
              "composer": "Marcin Pączkowski",
              "title": "Here comes a candle to light you to bed",
              "duration": 11,
              "sound_check": "10:15am - 10:45am",
              "performers": [
                "Marcin Pączkowski, motion sensors"
              ],
              "program-note": "The title of the piece comes from a nursery rhyme referenced in George Orwell’s book “1984”. Throughout the book the main character struggles to remember the poem’s ending, which is revealed to him at the key moment, right before he is captured: “Here comes a candle to light you to bed, here comes a chopper to chop off your head”.\n\nThis thread in the whole story resonated with me, as it touches on the volatility of one’s memory, with the backdrop of large-scale manipulation of recorded knowledge performed by the totalitarian regime in the book. While Orwell mostly deals with the memory that exists within humans and memory that’s written down, today we deal with omnipresence of recorded media of multiple sorts, particularly sounds, images, and videos. As we produce larger and larger amount of such records, not only through traditional books, audio records and movies, but also in social media, blogs, podcasts etc., I find it fascinating how we navigate this oversaturated space and how it is being transformed by both large-scale phenomena as well as targeted actions. In my piece I am seeking to explore these transformations by employing a machine learning model that embeds the memory of the piece. While being performed, the piece re-composes itself, as the model is being re-trained to embed new “memories” of the performance gestures.\n\nThis work is supported by the Department of Digital Arts and Experimental Media at the University of Washington, as well as eScience Institute with support from the Washington Research Foundation.\n\nThe piece is realized in 3rd order Ambisonics spatial sound format and employs custom motion sensors developed by the composer.",
              "tech_needs": "Here comes a candle to light you to bed is performed in real time using motion sensors connected wirelessly to a laptop with audio interface, positioned downstage from to the performer. Sensors, laptop, and audio interface are provided by the composer/performer. No live audio input is used in the piece, all sounds are synthesized. The piece can be adapted to a variety of speaker setups, as it is generated in real time in the Ambisonics format and decoded for any number of speakers.\n\n**Speaker setup:**\n\n* Minimum: 2 (stereo)\n* Recommended: 4-8, surrounding the audience\n\n**Possible output connectors:**\n\n- 2-8x TRS\n- 2-8x XLR\n- Dante (Ethernet)"
            }
          ]
        },
        {
          "title": "Workshop Session 1",
          "time": "2:45pm - 3:45pm",
          "type": "workshop",
          "workshops": [
            {
              "title": "SPRAWL 3.0",
              "facilitator": "Henrik von Coler",
              "abstract": "SPRAWL 3.0 is a new incarnation of a network-meta-instrument that has been developed since 2017. It is used for composition and performance of experimental electroacoustic music and in computer-music education. The recent version features 16 Access Points (AP) each consisting of a Raspberry PI 5, 10” touchscreen, audio interface and loudspeaker. All APs are peers in a server-less local area cluster, using JackTrip for a fully connected mesh audio network and an overlay network to ease OSC communication. SuperCollider (SC) serves as the main programming environment for creating configurations and AP software for improvisation and compositions. Audio can be flexibly routed though matrices in SC on each Access Point and GUIs allow musicians to interact with the meta-instrument. The basic idea behind SPRAWL is to explore different system configurations which are inherently connected to musical concepts. Some may use the system with additional musical instruments or microphones connected to each AP, while others work without additional input and create sound within SuperCollider. To make the reconfiguration of this complex system fast and robust, SPRAWL relies on professional admin-tools like Ansible to deploy concepts and configurations with a single command.\n\nIn a hands-on workshop/demo we show the capabilities of SPRAWL with 4-8 APs. Participants will create simple configurations and concepts from scratch by programming SC nodes on the Access Points and connecting the network ‘manually.’ They will also deploy existing concepts with Ansible, showing how advanced tools ease the use of a complex system.\n\nThe workshop is relevant to the following symposium topics: computational creativity, music performance, interactive music systems.",
              "location": "Location TBA",
              "tech_needs": "We will bring SPRAWL with all the components. This includes 8 Access Points (can be less or more, depending on venue and demand) with speakers and all cables. We need tables and some help with power supplies and power strips. The room for the workshop should be large enough to create a circle or horseshoe setup with the tables."
            },
            {
              "title": "Live4Life",
              "facilitator": "Christophe Lengelé",
              "abstract": "The workshop is based on the demonstration of  this spatial performance tool : https://github.com/Xon77/Live4Life. In addition to the \"Live For Life programme\", I will simultaneously show a new programme that I am currently developing, which integrates Tidal Cycles, SuperCollider, and various extensions quark, including SuperDirtMixer. This system features a GUI and controllers that execute command lines in Tidal Cycles and control multiple patterns and their variables (tempo, duration, synthesis, effects, or functions in patterns. It functions as a form of live coding DJing, where code lines replace traditional samples or records. The interface is still in development, with a public release planned for January 2025. An exemple of the workshop is summarized in the following link.",
              "location": "Location TBA",
              "tech_needs": "A quadriphonic system at the minimum to show spatialization, and a video-projector"
            }
          ]
        },
        {
          "title": "Paper Session 2",
          "location": "Location TBA",
          "time": "4pm - 5:20pm",
          "type": "paper",
          "presenters": [
            {
              "author": "Evan Murray",
              "time": "4pm",
              "remote": false,
              "title": "SCKinect",
              "abstract": "SCKinect is a SuperCollider plugin which allows users to interact with the Kinect v2 sensor in the server and language. Its core implementation contains a UGen called “Kinect,” designed to output human skeleton joint-tracking data to control buses. It will also include primitives designed to facilitate user interaction with Kinect devices. One example would be for users to be able to post all the available Kinect devices to the post window in SuperCollider, similar to how other human input devices work.\n\nThe shared benefit of the Kinect and SuperCollider is their ability to facilitate natural user interactions and artistic expression. The interpreted nature of SuperCollider and the server-language duality allows users to more-naturally communicate what they want to do, enabling them to focus on artistic expression. With the addition of the Kinect UGen, performers can interact with SuperCollider directly through their movement in an agent-less manner.\n\nThis project relates to interactive music systems and plugins for scsynth. Additionally, the GPU and CPU are both used here for visual and audio processing respectively. Although the GPU doesn’t directly provide audio, it is somewhat related to heterogeneous computing, as the CPU and GPU are both running the scsynth."
            },
            {
              "author": "Gerard Roma",
              "time": "4:20pm",
              "remote": true,
              "title": "Sound Matching",
              "abstract": "Sound matching is a classic problem in sound synthesis, originally posed for controlling FM synthesis. Given a target sound, the problem is to find the parameters that a synthesizer could use to synthesize the target. It is common to distinguish between in-domain targets (sounds originally generated by the same synthesizer) and out-of-domain targets (sounds coming from other sources). In recent work, I explored using SuperCollider to create synthesizer ensembles which were paired with ensembles neural network regressors to match out-of-domain sounds.\n\nThis paper will explore a more practical use case, using the Fluid Corpus Manipulation toolbox  to incorporate sound matching into purely SC-based creative workflows. Given a synthesizer designed to produce a range of sounds, an out-of-domain sound can be used to predict a set of parameters. It is up to the user to design the synthesizer and target for more predictable or more serendipitous results. The paper will consider the case for real-time matching as well as for offloading some of the parameters, such as pitch and amplitude,  from the regression problem."
            },
            {
              "author": "Tom Hall",
              "time": "4:40pm",
              "remote": true,
              "title": "A Case Study of Music Glyph Notation in SuperCollider using SMuFL fonts",
              "abstract": "*Keywords: extensions of sclang, live coding, multimedia music systems.*\n\nThis paper describes a dynamic ‘live’ music notation project using common practice Western notation (CPWN) ‘glyphs’ natively within SuperCollider. The project builds on work presented in a new co-authored ‘Notations’ chapter in the forthcoming 2nd edition of ‘The SuperCollider Book’, and aims to avoid any interaction with third-party music software common to related projects. A barrier to native SuperCollider implementation begins with displaying music glyphs. Since a specialist music font is required, glyphs cannot usefully be pasted as Unicode UTF-8 into an IDE. The ‘MITHUnicode’ dependency helper class translates between Unicode binary and hex encodings so that non-ASCII characters can be easily referred to and displayed in SuperCollider. Using Unicode identifiers and working specifically with music fonts, the W3C initiative ‘SMuFL’ (Standard Music Font Layout) enables font interoperability between different notation softwares. Glyphs comprising an extended CPWN superset are given unique Unicode code points as well as metadata to assist with complex positioning considerations for music display.\n\nCombining these approaches, the class ‘THNoteViewer’ enables the dynamic presentation of CPWN notes and complex chords within a SuperCollider Window in a format similar to Max software’s ‘nslider’ Object. To do this requires determining the correct placement of music noteheads and music glyphs such as ‘accidentals’ (flats, sharps etc). To avoid collisions between glyphs, SMuFL metadata is imported into the class, and dependency classes using computational geometry such as the ‘Graham scan’ algorithm and the Separating Axis Theorem are used to avoid glyph collision. Using a MVC design, THNoteViewer employs these approaches to display CPWN musical glyphs natively within SuperCollider in a manner that is both dynamic and responsive, whilst obeying complex engraving rules overlooked in some other related software environments."
            },
            {
              "author": "Fellipe M. Martins",
              "time": "5pm",
              "remote": false,
              "title": "An Exploratory Analysis of SCTweets Classification and Similarity",
              "abstract": "In the field of electroacoustic music, few artists and composers have published detailed information about their processes, and an even smaller number have provided a comprehensive blueprint, code, or instruction set that allows for a facsimile reconstruction of their work. With the advent of digital audio and the rise of creators making music exclusively on personal computers, it has become possible to track numerous details of the composition process when the software files are available. The use of programming languages for music composition has opened up a unique avenue for analysis, enabling musicologists to evaluate more clearly the relationship between sound results, high-level extracted features, and the tools and techniques used in their creation. An exceptional case is the SCTweets — 140-character SuperCollider code snippets that typically comprise compact sound procedures or dense compositions, shared on the microblogging platform Twitter (now X). We discuss the initial results of a timbral exploratory analysis on a specific set of SCTweets: tweets shared by Fredrik Olofsson on the repository sccode.org. Using the FluCoMa toolkit, we evaluate how timbral similarity might correlate with auditory criteria proposed by Pierre Schaeffer's theory. Additionally, we reveal various challenges and insights when correlating the usage of UGens and the provided timbral map. We selected Olofsson's SCTweets due to their unique use of the SuperCollider language, which often features codes utilizing only one number (666, 42, 1, 7), one type of UGen (SinOsc, Saw, LFCub), and codes that deliberately push DSP procedures to their limits to create new sounds, while adhering to the restriction of using only vanilla SC. We conclude the text with a discussion about the novelty of this analytical technique, presenting our findings and potential pitfalls."
            }
          ]
        },
        {
          "title": "Concert 2",
          "location": "Location TBA",
          "time": "7:30pm",
          "type": "concert",
          "works": [
            {
              "composer": "Thor Madsen",
              "title": "Thormulator",
              "duration": 10,
              "sound_check": "5:15pm - 5:45pm",
              "performers": [
                "Thor Madsen, Thormulator"
              ],
              "program-note": "4 years ago I began work on Thormulator with no prior knowledge of SuperCollider or programming in general. I had two objectives:\n\nFirstly, I wanted to create an app I could improvise with - an app which would feed me back meaningful musical responses in direct reaction to my guitar playing.\n\nSecondly, I was imagining ways of playing into a sound, i.e., shaping the sound of other instruments with my playing in the same way you can shape a bag of rice with your hands.\n\nOne can summarize my goal as designing a system which:  \n- Follows the performer in real time  \n- Has a sufficient number of responses to create a wide range of possible outputs  \n- Responds in a way that feels meaningful to the performer  \n- Allows the performer the maximum amount of musical freedom and range of expression  \n- Works without the need to use external controllers\n\nIn using such a system, I wanted to see if it is possible to establish a feedback loop between performer and machine which can inspire a new way of interacting with performance software, possibly a new type of music.\n\nhttps://thormulator.com/  \nhttps://thormadsen.com/",
              "tech_needs": "PA system with 2-channel input (L-R). 2 1/4\" jack - XLR cables for connecting my sound card to the stage box. Projection of the GUI of Thormulator on a screen behind me/next to me. HDMI or USB-C connectors to my HUB."
            },
            {
              "composer": "Gaël Moriceau",
              "title": "T-Stick",
              "duration": 7,
              "sound_check": "4:45pm - 5:15pm",
              "performers": [
                "Gaël Moriceau, T-Stick"
              ],
              "program-note": "This performance, for T-Stick and FM synthesis, offers an immersion into a synthetic soundscape inspired by the calm moments and storms of nature. The sound textures, in constant metamorphosis, evolve from delicate layers to gusty winds, providing a dynamic and immersive auditory experience.",
              "tech_needs": "Duration: 6-7 minutes.  The author will be performing the piece on stage. The following items will be supplied by the author:\n\n- Laptop (x1)\n- T-Stick (x1)\n- Router (x1)\n- Audio interface with stereo output (1/4\" jacks x2).\n\nThe author asks organizers to provide:\n\n- Table approximately 1 meter by 1 meter (x1)\n- Music stand (x1)\n- Music stand lamp (x1)\n- Extension cord for connecting the laptop (x1)\n- 1/4-inch jack cables (x2) to connect to the mixing console (the setup is stereo).\n- Loudspeakers (x2) connected to the mixing console (or directly to performer’s interface if no console is available)\n- Stage monitors (x2)\n- Moderate lighting during the performance The table with the laptop and interface should be positioned on stage or in close proximity for easy accessibility at the start of the performance.\n\nMore details in: https://drive.google.com/drive/folders/1I1kLqbyBRbKaovK60wRQSQBlNrK8cbpj "
            },
            {
              "composer": "Dennis Scheiba",
              "title": "Infinite Movement",
              "duration": 6,
              "sound_check": "4:15pm - 4:45pm",
              "performers": [
                "fixed media"
              ],
              "program-note": "Sound as movement in space is immanent, just as the movement of change is immanent in our society.\n\nA sonic equivalent of a social movement of eternal growth and optimization can be seen in the Shepard sound: an acoustic illusion that suggests an eternally rising tone.\n\nThis sound moves from the back of the past into the auditorium, where this acoustic illusion is deconstructed and takes over the space - one is in the center of the movement and perceives the breaking apart of the illusion through squeaking, suspension, stretching and compression of the sound.\n\nAfter a while, the curtain of a Shepard sound appears again, pulling further forward, whereby the sound has now taken on a downward movement that pulls you forward into itself.\n\nIn the end, the question remains as to what effect and possibilities an illusion creates that enters the space of tension between consciousness and perception, reflecting on the illusion of psychoacoustics.\n\nThe spatialization of Infinite movement is not to be understood as a mere effect of a spatial positioning of sounds, but interweaves with the narrative perspective of past, present and future, growth and decline.\n\nThe composition does not follow the classic spatialization technique, which suggests a reference point in a sweet spot where the localization of sounds works best.\n\nInstead, all listeners are in the same situation in the same space - from back to front, from top to bottom, from quiet to loud - and thus form an acoustic community.\n\nInfinite Movement won the Sound Cinema 2024 competition in the free category.",
              "tech_needs": "5 channel audio setup would be nice, but 4 channel audio setup would also be sufficient. It would be nice if you could provide an audio interface."
            },
            {
              "composer": "Matt Wellins",
              "title": "Improvisation",
              "duration": 13,
              "sound_check": "3:45pm - 4:15pm",
              "performers": [
                "Matt Wellins, electronics",
                "???"
              ],
              "program-note": "Program Note Needed",
              "tech_needs": "I'd be very happy to use a multichannel PA system, a table, two chairs - otherwise we are more-or-less self-sufficient."
            },
            {
              "composer": "Kyle Shaw",
              "title": "Rock Music",
              "sound_check": "3:15pm - 3:45pm",
              "duration": 9,
              "performers": [
                "fixed media"
              ],
              "program-note": "*Rock Music* is an 8-channel fixed-media composition. Supercollider provided all of the DSP needs for this piece -- granular synthesis UGens, resonant filter UGens, FFT UGens, wavetable synthesis, on top of writing my own functions to compose the piece's gestures and writing my own reverb SynthDef.",
              "tech_needs": "I have 8 mono audio files ready to be routed to an 8.1 surround setup."
            },
            {
              "description":"**Improvisation** by de_umbris.idearum",
              "composer": "de_umbris.idearum",
              "title": "Improvisation",
              "sound_check": "2:45pm - 3:15pm",
              "duration": 13,
              "bios":["James Annett"],
              "sound_checkers":["James Annett"],
              "performers": [
                "James Annett, electronics"
              ],
              "program-note": "I started using SuperCollider during the pandemic initially as a way to explore compositional structures using computers. This eventually led me to develop a performance practice of improvising in real-time with my software. I use a combination of methods including generative, aleatory, loop-based, gestural control, live coding, synthesized and sampled sound as well as various strategies of remembering and forgetting using neural networks.\n\nde_umbris.idearum is the name of my solo improvised electronic project. For me improvisation relates to embodiment and memory and the name was chosen in reference to Renaissance esotericist Giordano Bruno. As I was beginning to write code, I was also engaged in a choreographic practice involving reflective writing which also influenced me (notably in my desire to work with multi-channel audio).\n\nThis performance is specific to this occasion and dedicated to Mark Molnar.",
              "tech_needs": "I will need 8 channels ADAT or XLR. Walter will bring an amplified drum kit and will require a mic for his amp and a splitter to send the signal to my laptop for processing."
            }
          ]
        }
      ]
    },
    {
      "day": "Friday, March 14, 2025",
      "events": [
        {
          "title": "Coffee",
          "location": "Location TBA",
          "time": "8:30am - 9am",
          "type": "coffee"
        },
        {
          "title": "Keynote Address: Lucile Nihlen",
          "location": "Location TBA",
          "speaker": "Lucile Nihlen",
          "time": "9am - 10am",
          "type": "keynote"
        },
        {
          "title": "Paper Session 3",
          "location": "Location TBA",
          "time": "10:20am - 11:40am",
          "type": "paper",
          "presenters": [
            {
              "author": "Luis Alfonso Tamagnini",
              "time": "10:20am",
              "remote": false,
              "title": "The code as the expression of its own vitalism",
              "abstract": "*Keywords: computational creativity, music performance, interactive music systems, live coding, simulation, software studies, media theory*\n\nIn her book My Mother Was a Computer: Digital Subjects and Literary Texts, literary theorist and critic Katherine Hayles posits that; as computers are increasingly understood, and modeled after “expressive mediums” like writing, they begin to acquire the familiar and potent capability of writing not merely to express thought but actively to constitute it.\n\nThe code of the future will become more like the writing of the past – or rather, in the future there will be an as-yet-concealed hybrid of code and writing.\n\nIt is on this path on which I write this self essay about how I wrote and how I write computer music code. In it, I wonder about how formal language permeates the concepts of score, musical instrument, composition and performance among others. These structures become not only an expression of an abstract process or thought, but also an expression of itself, or of its own \"vitalism\"\n\nFurthermore, I allow myself to explore the live coding technique as a way to interact with such complex entities."
            },
            {
              "author": "Marcin Pietruszewski",
              "time": "10:40am",
              "remote": false,
              "title": "The New Pulsar Generator (nuPG)",
              "abstract": "Abstract Needed"
            },
            {
              "author": "Steph OHara",
              "time": "11am",
              "remote": true,
              "title": "Live-Coding and AI Assistance for Dynamic Musical Instrument Design in SuperCollider",
              "abstract": "This research explores live-coding techniques for developing accessible, interactive musical instruments within SuperCollider. It focuses on an enhanced real-time co-design process providing instantaneous adaptation to users' evolving needs, particularly for music students with disabilities. The approach integrates dynamic code manipulation, AI assistance, and the adaptation of Airsticks, a gestural musical instrument."
            },
            {
              "author": "Dennis Scheiba",
              "time": "11:20am",
              "remote": false,
              "title": "Stecker",
              "abstract": "Stecker is a project which enhances the sound generating nature of SuperCollider by providing a native way of distributing and receiving low-latency audio and data-streams over the internet within SuperCollider using WebRTC.\n\nThis is achieved by providing a set of  UGens which can receive and transmit such signals and also a web server, which takes care of the administration and distribution of said signals and allows users to listen or generate streams from within their browser.\n\nStecker is inspired by JITLib and live coding in general, and is also intended to serve as a platform by providing \"community radio stations\" which allow for on-the-fly remixing as well as establishing and exploring new places for acoustic performances and sources."
            }
          ]
        },
        {
          "title": "Concert 3",
          "location": "Location TBA",
          "time": "1pm",
          "type": "concert",
          "works": [
            {
              "description": "**Improvisation using *Lossy Codecs*** directed by Derek Worthington",
              "composer": "Derek Worthington",
              "title": "Lossy Codecs",
              "duration": 14,
              "sound_check": "12:15pm - 1pm",
              "performers": [
                "James Annett, viola",
                "Dustin Donahue, percussion",
                "Kerrith Livengood, flute",
                "Maxwell Miller, guitar",
                "Mason Moy, tuba",
                "Hyunkyung Shin, contrabass"
              ],
              "program-note": "Lossy Codecs is a system for directed improvisation that utilizes a dynamic graphic score created in real time, using a program built in SuperCollider. The program allows the director to control the changing values of various musical parameters, using TVs or a projector to display their screen to the performers. The performers improvise material based on their interpretation of this information, leading to unique sonic ecologies for each performance. The goal of this system is to merge methods and aesthetics from electronic music creation with live ensemble performance. One idea that inspired its creation was to be able to ‘play’ an ensemble as if it were a stochastic electronic instrument, controlling high-level parameters to shape the texture and form, while the details are supplied by live musicians instead of a program. But this idea must be balanced by a consideration of the improvising musicians’ freedom, creating an interesting tension and feedback process, where the director can react to the musicians as much as the other way around.\n\nThe system begins by defining a set of 17 musical parameters. Some are straightforward (Volume, Lyricism), while others are more abstract or ambiguous (Chunkiness, Thematicness). The values of the parameters are communicated by the height of colored bars, allowing the performers an intuitive approach to parsing the information. The director has control of what parameters are given at any time, and can set their values directly, assign goal values and transition times, or map them to independent LFOs, allowing complex and multidimensional textural progression.",
              "tech_needs": "There are two setup options for this performance: 1) Two TVs with HDMI or USB input on a low table, angled so all musicians can see  (If HDMI cables and a splitter can be provided, I won’t need to bring my own) 2) A projector with HDMI or USB input. This will only work if there is a suitable surface on which to project that the ensemble can see clearly while performing; the audience does not need to see it.  For either of the above methods, the following will also be needed: A small or medium table/surface at standing level for the director (to hold laptop and 2 midi controllers) Chairs for musicians and one shared music stand for every two performers (for reference sheets) Power strip  I will provide my own laptop and midi controllers"
            },
            {
              "composer": "Bruno Ruviaro",
              "title": "Notoligotoma hardyi",
              "duration": 13,
              "sound_check": "11:45am - 12:15pm",
              "performers": [
                "Dustin Donahue, percussion"
              ],
              "program-note": "Notoligotoma hardyi is a piece for solo percussion and live-electronics composed by Bruno Ruviaro (composer) in collaboration with Janice Edgerly-Rooks (biologist) and Dustin Donahue (percussionist). The combination of music and biology might seem surprising at first, but when one considers the type of data Edgerly-Rooks collects, bringing these fields together makes sense. Since 2005, Janice and her students have been quantifying silk spinning behavior of a few dozen species of insects called Embioptera. They spin silk by stepping with their front feet, packed with silk glands, around their bodies as they secrete silk to build a domicile where they gain protection against the elements. Individuals display thousands of spin-steps to complete construction. While analyzing long data sets, she noticed that sometimes the data reminded her of musical patterns. Ruviaro looked at the data and wondered what it would sound like if spin-steps were recreated for percussion instruments. He analyzed and transcribed over 5000 spinning steps of an Australian species, Notoligotoma hardyi—hence the composition’s name. The data were then made audible through sonification techniques (sonification is the use of non-speech audio to perceptualize data). Individual steps were assigned to percussion instruments and a score was created representing the data. Performer Dustin Donahue helped not only in the process of carefully choosing instruments, but also brought his own rhythmic imagination to the otherwise rhythm-less data. In addition to using acoustic instruments, Ruviaro added live-electronic sounds to represent the growing silk structures that result from the insect’s choreography. The piece was created using SuperCollider on Ubuntu Linux.",
              "tech_needs": "This is a piece for one percussion player and live-electronics (stereo). If accepted, both composer and performer would be present at the symposium to perform it. We would bring our own set of percussive instruments, as well as contact mics and other equipment necessary to run the piece. We need to send a stereo signal out from our audio interface into the venue's stereo system. Depending on size of venue, slight amplification of percussion instruments might be advisable."
            },
            {
              "composer": "Hyunkyung Shin and Henrik von Coler",
              "bios": [
                "Hyunkyung Shin",
                "Henrik von Coler"
              ],
              "title": "2CUBES",
              "duration": 14,
              "sound_check": "11:15am - 11:45am",
              "performers": [
                "Hyunkyung Shin, Contrabass & ARCube",
                "Henrik von Coler, Modular Synthesizer & ARCube"
              ],
              "program-note": "CuboiDuo is an electronic duo performance utilizing two ARCube(s), an augmented reality (AR) interface designed for three-dimensional spatial control. The ARCube enhances engagement and immersion by being placed alongside physical control devices. Shaped as a cube with four 3-centimeter-diameter spheres, the ARCube enables spatial manipulation through AR. The virtual scene is generated using Unity Engine, and the cube serves as a scaled model representing the physical dimensions of the spatial audio system.\n\nTwo hand gestures, Grab and Pinch, enable intuitive interaction with the cube: the Grab gesture allows performers to move and rotate the cube, while the Pinch gesture controls the movement of the spherical objects within it. The cube’s Y-axis rotation is constrained to maintain stability during performance, and it can be conveniently placed next to the performers.\n\nFor virtual source positioning, OSC messages are transmitted at 100 Hz over WiFi to a Linux-based rendering server running SuperCollider-based spatialization software. Each performer’s instrument—double bass and modular synth—connects to four sound synthesis processes on the same server. Through the ARCube interface, performers can manipulate the individual sound characteristics of each process via gesture control.\n\nThe audio streams are routed to four Higher Order Ambisonics (HOA) encoders, generating four virtual sound sources that are decoded and projected through a quadraphonic loudspeaker setup. The real-time inputs from the double bass and modular synth undergo spatialization and sound transformation in real time, producing experimental electronic music.\n\nThe performance emphasizes avant-garde improvisation, blending the extended techniques of the double bass with the rhythmic, electronic sounds of the modular synth. This interplay creates an experimental stage where acoustic sounds are electronically transformed and layered with modular synth’s dynamic textures, resulting in a uniquely immersive electronic music experience.",
              "tech_needs": "We will bring PCs for rendering and audio processing, along with network cables and a router, as well as two HMDs for the performers. We require four loudspeakers, a double bass with a bow, and eight XLR cables to connect instruments to the rendering PC. Additionally, we need a table for the modular synth setup and a projector or screen with HDMI input for the live-streamed AR scene. Proper lighting will also be helpful to enhance the performance."
            },
            {
              "composer": "Mason Moy",
              "title": "Copper Ouroboros",
              "duration": 11,
              "sound_check": "10:45am - 11:15am",
              "performers": [
                "Mason Moy, tuba"
              ],
              "program-note": "Program Note Needed",
              "tech_needs": "I would only require a microphone to amplify my tuba performance. I will bring all other materials (speaker, interface, microphone for creating feedback). "
            },
            {
              "composer": "Maxwell Miller",
              "title": "Antithesis (this is the part where I scream)",
              "duration": 9,
              "sound_check": "10:15am - 10:45am",
              "performers": [
                "Maxwell Miller, guitar & voice"
              ],
              "program-note": "Antithesis (this is the part where I scream) is the embodiment of the frustration I sometimes feel dealing with a split sense of self. It explores feelings of inevitability and anger, with me grappling with the electronic sounds and fighting to retain voice as a performer amidst a thick and sometimes overwhelming texture. This piece is quite open in its structure, asking the performer to create gestures of increasing intensity until it reaches its point of climax, a full scream, until releasing that energy back into silence. This piece forgoes the use of video and is performed in near-darkness, symbolic of the internal experience it portrays.",
              "tech_needs": "Inputs (from FOH): none Outputs (to FOH):      Audio Interface Left (XLR)      Audio Interface Right (XLR) Power requirements: 1 grounded outlet.  Optional: Music stand  I can provide all other equipment.  My stage setup is as follows:      Laptop and iPad on laptop stand      Guitar amp modeler and audio interface/power strip on floor      Dynamic mic and mic stand"
            }
          ]
        },
        {
          "title": "Workshop Session 2",
          "time": "2:45pm - 3:45pm",
          "type": "workshop",
          "workshops": [
            {
              "title": "The Absolute Relativity",
              "facilitator":"Alberto de Campo & Bruno Gola", 
              "facilitators": ["Alberto de Campo","Bruno Gola"],
              "abstract": "The Absolute Relativity workshop invites 5-7 symposium participants to prepare a joint experimental performance: In advance of the symposium, we will send an open call to the participants inviting people to sign up and prepare two to three sound processes to be integrated in a collective performance environment written in SuperCollider, the NTMI. This environment is designed for exploring complex sound processes by intuitive interaction, and has recently been expanded for finely gradable layers of shared influence between multiple players.\n\nWe will make a collection with all sound processes shared by the participants, and before the on-site workshop we ask them to practice with this environment individually to get to know each other’s sound worlds. In the on-site workshop sessions all participants bring their own interfaces and setups running the NTMI environment, and we explore the different possibilities of mutual influence. To conclude the experiment, we collectively play with this environment in the conference concert program.",
              "location": "Location TBA",
              "tech_needs": "For the workshop we would need:\n\n- chairs+table space for all participants (up to 7 + 2 tutors) with their laptops and MIDI controllers.\n- ideally individual speakers for each participant\n- alternative: mixer with enough stereo inputs (9 stereo in) + stereo monitors\n- a classroom-size HDMI screen or a projector"
            },
            {
              "title": "Supriya",
              "facilitator": "Joséphine Oberholtzer",
              "abstract": "Supriya is a Python API for SuperCollider - an alternative language client - in active development for the past ten years. This workshop will demonstrate usage of Supriya via an interactive Jupyter notebook, detailing its common “context” interface for realtime and non-realtime synthesis. We’ll examine Supriya’s support for both threaded and “asyncio” event-loop concurrency models, and walk through examples of unit generator graphs, clocks, and a basic pattern library. Throughout the demonstration, design decisions and underlying implementations will be emphasized. Time permitting, topics of unit testing, CI pipelines, and static typing will also be discussed.",
              "location": "Location TBA",
              "tech_needs":"Projector with USB-C input, stereo playback, 1/8” stereo input."
            }
          ]
        },
        {
          "title": "Time Allocated for Visiting Installations",
          "time": "4pm - 5:30pm",
          "type": "installations",
          "installations": [
            {
              "title": "Only footprints",
              "artist": "Drew Farrar",
              "location": "Location TBA",
              "tech_needs":"Either 1 large projector (preferred) or screen, stereo to 8-channel sound.\n\nFor sound:\n\n* Multi-channel interface (if possible)\n* 1/4 inch ts cables (1 per speaker) (as long as possible/reasonable for the space)\n* 2-8 speakers (plus stands)\n\nI can provide the following, though it would be nice to have these provided/a back-up:\n\n* 8-channel interface (mine is getting old)\n* HDMI cable (as long as possible)\n* Gaff tape\n* Ethernet cable (as long as possible)\n* Power extension cord (as long as possible)\n* Surge protector or other single-to-multi outlet power adapter (minimum 3 outlets)",
              "abstract":"_Only footprints_ is an interactive multi-media installation that utilizes SuperCollider, Arduino, and the Godot game engine. The piece explores concepts of the human influence on the environment, distortion, and decay. The work consists of an interactive video, stereo to 8 channel sound, and a chair with an embedded Arduino. When the seat is empty the audio consists of recordings of extinct birds and the video a rendering of a forest, creating a facsimile of a nature scene. When an audience member sits in the chair a process of spectral stretching and distortion begins, deforming the audio, while a custom written shader stretches the geometry of the video in real time. This work relates to both interactive music systems and multimedia music systems, and (aspirationally) hopes of future work either embedding sclang and scsynth into the Godot engine directly (for XR installation) or embedding scsynth/supernova and using Godot's native scripting language to interact with the server (alternative language clients for scsynth). To create this work I wrote an OSC library for Godot's scripting language (GDScript)."
            },
            {
              "title": "Return to Tomorrow",
              "artist": "Michael Webster",
              "location": "Location TBA",
              "tech_needs":"piece is written for speakers in a pentagon and either 1, 3 or 6 video-monitors but is adaptable.",
              "abstract":"_Return to Tomorrow_ is an opera from the Star Trek episode of the same name, using AI voices, written in Supercollider.  The episode concerns a voice without a body, emanating from a dead planet, ultimately opening up ideas  about the human/machine interface, and the nature of consciousness and connection. The show is here performed by the computer - the code being interpreted can be seen behind and through meme-like stills from the show. It’s a recreation of a piece from the past about a warning from the future... presented, hopefully, with humor and pathos.\n\nIn order to organize the music around speech-like rhythms, I built a kind of DAW that indexes all events not to beats/bars or minutes/seconds but instead to lines of text/syllables, allowing me to reflow all music (including the singing) by tapping out rhythms on my laptop, so the whole show retains a kind of parlando vibe and running time is very close to the original TV episode. Tools were written to build project files for the voice synth and interface with ffmpeg and Kitty to generate visuals."
            }
          ]
        },
        {
          "title": "Concert 4",
          "location": "Location TBA",
          "time": "7:30pm",
          "type": "concert",
          "works": [
            {
              "title": "Sigil II: Amistad",
              "composer": "Monte Taylor",
              "duration": 10,
              "sound_check": "6pm - 6:30pm",
              "performers": [
                "Doug O'Connor, saxophone"
              ],
              "program-note": "*Sigil II: Amistad* uses Supercollider to generate synthesized soundscapes in realtime, which a live saxophonist must interact with and respond to as they perform the music presented to them on the score. This interaction is at the core of the programmatic elements of the piece, which reflects on notions of cultural diversity and fusion as presented in José Parlá's Mural \"Amistad America\". Supercollider is also used to apply audio effects to the saxophone's signal to further color the interaction between the performer and computer.",
              "tech_needs": "One Condenser Microphone for Saxophone 2, 4, or 8 channel line-outs from my audio interface (I can be flexible on the number of channels if need be)"
            },
            {
              "title": "saccades",
              "composer": "Ted Moore",
              "duration": 16,
              "sound_check": "5:30pm - 6pm",
              "performers": [
                "Doug O'Connor, saxophone"
              ],
              "program-note": "A “saccade” is a rapid movement of the eyeball between two fixed focal points. During this brief moment, the brain hides this blurry motion from our perception. Once a saccade motion has begun, the destination cannot change, meaning that if the target of focus disappears the viewer won’t know until the saccade completes. If the field of vision is changing too quickly, the saccades may never be able to arrive at and focus on a target, instead, the objects in view are only perceived through peripheral vision. \n\nThis idea is imitated by the sound and video presented in the piece. It also serves as a metaphor for the density of information and high entropy experiences we’re constantly trying to cope with. A scroll on social media, smartphone alerts, big data, technological advancements and predictions, the abundance of choices in the grocery aisle.\n\nThe processes used to create *saccades* embody this density of information–and the constant struggle to track it all. Large datasets of audio analyses derived from the tape part are sent to neural networks and dimensionality reduction algorithms to find patterns and then visualize and sonify what is found. A plethora of variations on visual themes are created by the combinatorics of stochastically triggered visual synthesis modules and processing effects. Computer vision analysis adds layers to our visual and aural perception–tightly binding together the visual and auditory elements. In the final movement, source video of an eyeball is morphed via a convolutional neural network creating eyeballs that feel both simple and obvious, but also slightly make my stomach squirm.",
              "tech_needs": ""
            },
            {
              "description": "**Improvisation** by Kerrith Livengood",
              "composer": "Kerrith Livengood",
              "title": "Improvisation",
              "sound_check": "5pm - 5:30pm",
              "duration": 20,
              "performers": [
                "Kerrith Livengood, electronics & instruments"
              ],
              "program-note": "I am an improvising performer who is currently developing an ambitious multi-component performance array in SuperCollider. This elaborate setup includes unique live processing effects combining pitch-tracking and looping in complex chaotic ways. The project allows for integration of live instruments, MIDI instruments using VST plugins, and live coding into a single performance by a single performer. I've been performing live improv sets using different versions of this array over the last few months, with many different combinations of synthesizers, instruments, and musical toys. I'd like to present a performance using this multi-component array, as well as possible discussion of collaborative and multimedia possibilities, especially dance and live video improvisation.",
              "tech_needs": "I would require a condenser microphone and stand, and stereo connection to a PA. This piece could easily be modified for a multichannel presentation. I will provide my own interface, related cables, and musical instruments. I'm also willing to perform works by other conference attendees (on flute or miscellaneous)."
            }
          ]
        }
      ]
    },
    {
      "day": "Saturday, March 15, 2025",
      "installations":[
        {
          "title": "Gamang",
          "artist": "Sarah Lecompte-Bergeron",
          "location": "Location TBA",
          "tech_needs":"Equipment provided by the venue\n\n**Needed:**\n\n* 4x speakers + stands\n* 1x subwoofer\n* 5x ¼ jack cables 3x > 25 feet, 2x > 50 feet (audio interface to speakers)\n* 1x XLR cable > 25 feet (instrument to audio interface)\n* 1x projector + HDMI cable > 10 feet (laptop to projector)\n* 4x multisocket extensions\n* 1x table/other to conceal the laptop and audio device\n* 1x carpet/cushion for visitor sitting at the instrument\n* Space where constant dark lighting is possible\n* Space where structures are available to hang 4 lightweight projection surfaces made from tulle\n* Possibility to borrow stepladder/other to install the projection surfaces\n\n**Optional:**\n\n* cushions and chairs for visitors (sits 4-10 people)\n* plants\n* bowl of water to install a pump (water sounds)\n* authorization to burn incense (this is not possible)\n\nhttps://udemontreal-my.sharepoint.com/:f:/g/personal/sarah_lecompte-bergeron_umontreal_ca/ElcsmIfr6T5LrGAF9lwdY0QBGrmia0lmqz3C64xt6igfZg?e=lhQnOw",
          "abstract":"_Gamang_ is an interactive installation inspired by the feeling of being separated from friends and loved ones. « Gamang » is a type of ghost in Balinese folklore that can be encountered at night in isolated spaces.\n\nThe haunting spirit recalls the absence of people who were once close that we continue to feel as a presence. This installation was motivated by the desire to recreate the feeling of playing with peers in a gambang ensemble, a type of Indonesian gamelan.\n\nThe musical motifs are algorithmically generated from transcriptions of unique performances. An automatic music transcription algorithm was developped to ease the process. The public is invited to engage with digital ghost musicians by playing a « gangsa » (gamelan metallophone) to guide the patterns and be part of the virtual ensemble. The ghosts react to the human player sitting within them using a computer vision algorithm designed to follow anticipated actions, akin to an improvised human performance of gambang. The patterns are generated as the player improvises on the instrument.\n\nA SuperCollider class was written to better suit balinese gamelan musical thinking while serving sound experimentation. Musical concepts unique to gamelan are embedded in the synthesis system and the interface.\n\nThe ghosts materialise themselves as flickering firefly silhouettes floating on layered mesh, giving them dimension. By walking between the sound sources, different complementing parts of the Kotekan, a type of interlocking pattern idiomatic to gamelan music, are highlighted. When at rest, the sound essence of the ghosts roam the cemetary in a quadraphonic arrangement. This nocturnal atmosphere develops itself generatively within a field recording of a full night in a cemetary in the mountains of the village of Munduk."
        }
      ],
      "events": [
        {
          "title": "Coffee",
          "location": "Location TBA",
          "time": "8:30am - 9am",
          "type": "coffee"
        },
        {
          "title": "Keynote Address: Christof Ressi",
          "location": "Location TBA",
          "time": "9am - 10am",
          "type": "keynote",
          "speaker": "Christof Ressi"
        },
        {
          "title": "Paper Session 4",
          "location": "Location TBA",
          "time": "10:20am - 11:40am",
          "type": "paper",
          "presenters": [
            {
              "author": "Hyunkyung Shin",
              "time": "10:20am",
              "title": "ARCube",
              "remote": false,
              "abstract": "The study presents a SuperCollider-based spatial audio performance system that leverages an augmented reality (AR) interface, the ARCube. For real-time control and creative manipulation of spatialized sound, the system demonstrates how SuperCollider’s flexibility in handling complex synthesis and spatialization algorithms, combined with OSC (Open Sound Control) data from an AR interface, can be effectively harnessed for interactive and expressive live music performances.\n\nThe SuperCollider system is structured to receive high-frequency OSC messages (100 Hz) from the ARCube, which transmits 3D positional data (azimuth, elevation, distance) to dynamically control sound sources and spatial effects. Key scripts developed in SuperCollider include definitions for multi-channel spatial encoders and SynthDefs designed for various sound processes, such as pitch shifting, delay feedback, and spectral filtering. A central HOA (Higher Order Ambisonics) encoder processes the spatial positioning data from OSC messages, allowing the system to route audio sources through a 3D spatial field. SuperCollider groups and audio buses manage these HOA-encoded channels, enabling interactive manipulation of sound positions and effects through the user’s gestures with the ARCube.\n\nThe ARCube’s OSC data is parsed by SuperCollider and assigned to control buses. These control buses then map the received parameters to specific SynthDefs, such as grain effects and filters, enabling nuanced audio transformations directly influenced by the ARCube’s real-time positional updates. Gesture-based controls, such as Grab and Pinch, are mapped to functions like sound rotation, source movement, and synthesis parameter modulation, creating a tightly coupled feedback loop between performer gestures and sound transformations, fostering an intuitive interface for spatial audio control.\n\nTo validate the system’s efficacy, a creative performance was designed with trained performers. Through this performance, performers showcased extensive creative interaction and auditory perception using the AR interface. This highlights SuperCollider’s potential, with its flexible OSC integration and real-time DSP (Digital Signal Processing) capabilities, to significantly enhance creative expression in spatial audio performance. This work underscores SuperCollider’s suitability for complex spatialization and interaction scenarios, demonstrating a framework for AR-enabled spatial audio systems in live, immersive music performances."
            },
            {
              "author": "Joo Won Park",
              "time": "10:40am",
              "remote": true,
              "title": "Performing and Sharing Laptop Ensemble Repertoire",
              "abstract": "Laptop ensemble repertoire has a better chance for repeat performance if they have low technical, technological, and cost barriers. Composers can make and share such pieces by making easy-to-render SuperCollider codes and clear instructions. In this session, I will present two examples performed by multiple ensembles consisting of beginner SuperCollider users or electronic musicians."
            },
            {
              "author": "Scott Carver",
              "time": "11am",
              "remote": true,
              "title": "Declarative SuperCollider",
              "abstract": "Abstract Needed"
            },
            {
              "author": "Joel Ong",
              "time": "11:20am",
              "remote": false,
              "title": "Expanding the landscape of SC through multimodal art",
              "abstract": "At the symposium, I will present a series of works that make use of Supercollider (SC) to organize, sonify data and manage interactions through a variety of means. For example, Aeolian Traces (2016) was a multi sensory project exploring spatial sound and data visualization of narratives of human migration, including community focused work on the undocumented migrant population in Seattle. The system centrally run through SC uses geolocation datasets to drive wind currents through Arduino controlled cooling fans, networking with Processing for real time visualization, and spatial sound propagation through a low cost Ambisonic 16 channel setup. Similarly, in, Birdsong Diamond (2015-19) a user interface was built in SC to control the interactive component of the installation where visitors were invited to mimic birdsongs and given an eventual score for their attempts. More recently, the project untitled interspecies umwelten (2022-current) is an artistic research project exploring expanded and computer-mediated experiences of conversation with other species.  The project networks computer vision, LLM APIs to control interaction, text generation.  Responding to the question of the future of SC, these humble experiments propose to highlight the way SC can be instrumental as a central control system for multi-programmatic outputs.  I am more immediately a part of the user-group that has a limited but sustained experience with SC and moderate dexterity within its programming environment, so accessibility and longevity of these tools are one reason SC has remained a pivotal tool for my work. My aim at this conference would be to learn and adopt new approaches and in envisioning them within ongoing and new project, help shape its role in the future of creative systems and collaborative work."
            }
          ]
        },
        {
          "title": "Concert 5",
          "location": "Location TBA",
          "time": "1pm",
          "type": "concert",
          "works": [
            {
              "title": "The Absolute Relativity",
              "composer": "Alberto de Campo & Bruno Gola",
              "duration": 30,
              "sound_check": "11:15am - 12pm",
              "performers": [
                "Workshop Participants"
              ],
              "program-note": "The Absolute Relativity performance is the result of a workshop with 5-7 symposium participants. In this performance we explore the different possibilities of mutual influence in a collective performance environment written in SuperCollider, the NTMI. This environment is designed for exploring complex sound processes by intuitive interaction, and has recently been expanded for finely gradable layers of shared influence between multiple players.  In advance of the symposium, we will send an open call to the participants inviting people to sign up for a workshop and prepare two to three sound processes to be integrated into NTMI.\n\nDuring the concert we collectively play with this environment and each other sound worlds. The performance can be between 30 and 60 minutes. Depending on the available time we can show different influence topologies and strategies in the environment.",
              "rider": "Peabody is providing an 8-channel Focusrite",
              "tech_needs": "Depending on the venue's sound system possibilities we have different options:\n- the best option is one wide stereo pairs for every participant, for instance by patching everyone into a multi-channel system. This enables the audience loosely identify who is playing what, and have a nice immersive total spatial image of all performers.\n- an alternative is single individual speaker for each player.\n- a less optimal option is a mix down to a half circle of for speakers, for example. in this case we would need a mixer with enough input channels and flexible output routing."
            },
            {
              "title": "Metacortical Modulations",
              "composer": "Kosmas Giannoutakis",
              "duration": 18,
              "sound_check": "10:45am - 11:15am",
              "performers": [
                "Kosmas Giannoutakis, electronics"
              ],
              "program-note": "*Metacortical Modulations* explores a mode of post-human musical expression through an intricate coupling of biological signals, algorithmic processes, and community-driven creativity. Utilizing a Brain Control Interface (BCI), the performer captures and live-maps brainwave and muscle tone data to modulate parameters of SCTweets—concise code snippets composed by various authors and shared within the SuperCollider community. This performative framework integrates somatic and machinic data streams with the collective creative output of a global network of computer musicians in a hyper-dynamic, unpredictable and non-hierarchical fashion. The resulting soundscape traverses a spectrum of sonic textures, from noise and drones to rhythmic irregularities and fragile frequencies, all emerging from the interplay between the performer's physiological state, digital glitches, non-linear feedback and the diverse algorithmic expressions of the SCTweet authors. By eschewing traditional notions of individual authorship and musical control, this performance manifests a form of distributed creativity that blurs the boundaries between human intention, bodily processes, computational algorithms, and communal creativity, while celebrating the social and collaborative ethos of the computer music community.",
              "tech_needs": "- 8-channel sound system (requested) - class-compliant USB audio interface that supports 8 audio outputs (requested - otherwise I can provide mine which has only 6 working audio outputs) - HDMI projector, screen and lighting (requested) - OpenBCI EEG (provided) - Laptop (provided) - Akai MIDIMIX (provided) - Mechanical keyboard (provided) - Performance table (requested)"
            },
            {
              "description": "**Improvisation** by mod f",
              "bios": [
                "Maxwell Gong",
                "Alexander Wu"
              ],
              "sound_checkers": [
                "Maxwell Gong",
                "Alexander Wu"
              ],
              "composer": "mod f",
              "sound_check": "10:15am - 10:45am",
              "title": "Improvisation",
              "duration": 13,
              "performers": [
                "Maxwell Gong, electronics",
                "Alexander Wu, electronics"
              ],
              "program-note": "mod f is an electronic improvising duo of Alexander Wu and Maxwell Gong. In their music, the material world is distorted, physicality decoupled from sound waves; metaphors feed back into one another and into themselves. mod f constructs fragile moments of tranquility within chaotic systems and carves out dissenting spaces from the static humming of electric current.",
              "tech_needs": "All three of us need to sit near our stereo outputs. Usually, this means one in the front, one on the left, and one on the right. We each need two monitors to listen to the other two performers. We each have two TRS outputs and need to route through the mixer to give mono sends to the other two performers. Depending on the location and tech availability, we are willing to make adjustments to our ideal setup."
            }
          ]
        },
        {
          "title": "Paper Session 5",
          "location": "Location TBA",
          "time": "3pm - 4:20pm",
          "type": "paper",
          "presenters": [
            {
              "author": "Alfonso Fellipe Romagna",
              "time": "3pm",
              "remote": true,
              "title": "THE COMPOSITION OF INTERACTIONS IN ‘GUERREIRA DAS PEDRAS’ (2024)",
              "abstract": "*Keywords: SuperCollider, MediaPipe, openFrameworks, Creative processes, Interactive Systems*\n\nThis work presents the creative and compositional processes in the piece Guerreira das Pedras (2024) for saxophone, dance, video, and real-time electronics. In this paper, we prioritize an emphasis on the conceptual aspect, where we discuss the main artistic intentions and how the poetics of the piece influenced the choice of tools in the creative workflow involving interactive and multimodal systems. The text presents the artistic motivations, a description of processes and interconnections, a description of the computational model, as well as reflections on the relationship between aesthetic intentions and technological choices. Part of the methodology presented is inspired by the proposal of composer and researcher Marije Baalman, where some visual symbols were also used to exemplify the workflows and procedures carried out."
            },
            {
              "author": "Rohan Drape",
              "time": "3:20pm",
              "remote": true,
              "title": "Block SuperCollider",
              "abstract": "*Keywords: open-source programming language design, interactive music systems, alternative language clients for scsynth*\n\nBlock SuperCollider is an experimental visual programming language specialised for writing programs for the SuperCollider synthesiser. The system investigates eight areas of research: 1. rich-text editors for SuperCollider programs, 2. rendering program texts as interactive control surfaces, 3. browser based editors for the WebAssembly SuperCollider synthesiser, 4. tablet and pen based editors for SuperCollider programs, 5. environments for musicians who are non-programmers working with SuperCollider, 6. literate programming systems for SuperCollider programs, 7. simple systems for sharing SuperCollider programs with non-experts, and 8. bi-directional translation between notational systems. This essay provides a rationale for and overview of the Block SuperCollider system, describes its operation and implementation, reviews related work, and discusses directions for future research."
            },
            {
              "author": "Victor Zheng",
              "time": "3:40pm",
              "remote": false,
              "title": "ChessSynth",
              "abstract": "I propose to introduce ChessSynth, a framework built in SuperCollider for procedurally generating a soundtrack in real time to chess games. I will describe my methodology and algorithm design and then present some demonstrations of sample generations of chess games. This work touches upon algorithmic composition, data sonification, and potentially asks the question of using a non-musical entity such as chess as a factor in live performance of music."
            },
            {
              "author": "Derek Worthington",
              "time": "4pm",
              "remote": false,
              "title": "Polytempic Music with SuperCollider",
              "abstract": "I began composing polytempic music – music in multiple tempos simultaneously – using a ruler and pencil, measuring out relative bar lengths to line up parts. This was extremely labor intensive, and if I wanted to change any tempo relationships, everything would need to be completely redone. So I used SuperCollider to build a visual system to make the compositional workflow as smooth as possible.\n\nThe system creates timelines for each unique tempo stream and aligns them vertically, marking beats and measures horizontally. Tempos can be entered as a number or as a ratio with another part; the system takes care of the calculations and rescales the beats appropriately. Tempo changes can be added on any beat, or mapped to an LFO or user-defined envelope to vary smoothly over time. The alignment points between timelines can be changed as desired, automatically shifting everything horizontally. The length of each timeline can be adjusted, and a zoom feature allows detailed work on any small part of the full composition.\n\nRhythms may be entered with a mouse click. Highlighting and dragging, and augmentation and diminution functions, allow rhythms to be easily entered, edited, copied, and rearranged. A playback feature can provide simple clicks and boops, or send OSC signals to separate synthesizers in which melodies and chords may be programmed. This is not meant to produce ‘final’ audio that a listener would hear, but as a way for the composer to hear a simple representation of the rhythms and melodies they’re working with. This feedback is extremely useful to get a concrete sense of more oblique or complicated tempo relationships.\n\nFor this presentation I will discuss the system in detail and show examples of how it has been and could be used."
            }
          ]
        },
        {
          "title": "Roundtable Discussion: \"The Future of SuperCollider\"",
          "time": "4:30pm - 5:30pm",
          "type": "other",
          "location": "Location TBA"
        },
        {
          "title": "Banquet Dinner",
          "time": "5:45pm - 7:15pm",
          "type": "other",
          "location": "Location TBA"
        },
        {
          "title": "Concert 6",
          "location": "Location TBA",
          "time": "7:30pm",
          "type": "concert",
          "works": [
            {
              "description": "**Improvisation** by Sam Pluta and Bonnie Lander",
              "title": "Improvisation",
              "composer": "Sam Pluta and Bonnie Lander",
              "bios": [
                "Sam Pluta",
                "Bonnie Lander"
              ],
              "duration": 13,
              "sound_check": "7pm - 7:30pm",
              "performers": [
                "Sam Pluta, electronics",
                "Bonnie Lander, voice"
              ],
              "program-note": "Bonnie Lander and Sam Pluta have been performing in duo and other formations for the past 3 years. Their practice combines Lander’s virtuosic vocal technique with Pluta’s myriad synthesis and live-manipulation processes.",
              "tech_needs": "stereo sound to the house"
            },
            {
              "title": "Live for Life",
              "composer": "Christophe Lengelé",
              "duration": 10,
              "sound_check": "3:45pm - 4:15pm",
              "performers": [
                "Christophe Lengelé, electronics"
              ],
              "program-note": "This project is an electronic improvisation using SuperCollider-based synthesis and recordings from various electronic machines, including modular synthesizers, analog and digital synthesizers, and drum machines. The primary sound source consists of over 2 gigabytes of samples, containing more than 5,500 sounds, organized into multiple folders for real-time manipulation. The improvisation focuses on creating evolving soundscapes by combining pre-recorded samples and live synthesis.\n\nIn addition to the Live For Life programme (https://github.com/Xon77/Live4Life), I will simultaneously use a new programme that I am currently developing, which integrates Tidal Cycles, SuperCollider, and various extensions quark, including SuperDirtMixer. This system features a GUI and controllers that execute command lines in Tidal Cycles and control multiple patterns and their variables (tempo, duration, synthesis, effects, or functions in patterns. It functions as a form of live coding DJing, where code lines replace traditional samples or records. The interface is still in development, with a public release planned for January 2025.",
              "tech_needs": "I bring a laptop and controllers. I've got also a Motu M4 with 4 outputs. I can spatialize on 8 loudspeakers with the appropriate sound card, but can also spatialize in quadraphony or output on stereo."
            },
            {
              "description": "**Live Coding Performance** by Eli Fieldsteel",
              "composer": "Eli Fieldsteel",
              "title": "Live Coding Performance",
              "sound_check": "3:15pm - 3:45pm",
              "duration": 20,
              "performers": [
                "Eli Fieldsteel, electronics"
              ],
              "program-note": "This submission proposes an improvised live coded performance, which is listed as a topic on the Call for Contributions. Ideally, this performance would occur at a late-night venue (e.g. a restaurant/bar or similar), which provides an atmosphere in which a relatively long and slow-evolving performance is more contextually appropriate. If this is not an option, a performance on the concert stage is also acceptable. Most rehearsals and performances in the past have lasted for about 30 minutes. If this is deemed too long for any reason, the duration can be shortened by leaning away from free improvisation and preparing more of the performance code in advance.",
              "tech_needs": "I am prepared to perform in eight-channel surround and will bring an audio interface with eight 1/4\" TRS outputs. If this option is not available, a stereo performance is also possible."
            },
            {
              "title": "Microwebs",
              "composer": "Bruno Gola",
              "sound_check": "2:45pm - 3:15pm",
              "duration": 15,
              "performers": [
                "Bruno Gola, electronics"
              ],
              "program-note": "Microwebs is a hybrid between live improvisational performance and installation for dynamic multichannel setups using audience's mobile phones as a sound system. The performance explores the relationship between cybernetic music, intuitive playing with macro-controls and influence, while also expanding those ideas into the audience participation via sound and space activation. During the live performance the audience is invited to explore the intra-action between sounds while the performer improvises with influence parameters. The resulting experience is a combination of relations between sound processes internally feedbacking in webs of sounds, the audience exploring the space with their mobiles as sound sources, and the performer influencing back the sound processes.",
              "tech_needs": "The performance can last anytime between 15 to 30 minutes.  - table space for computer + small midi controller - internet availability (preferably if the location has good mobile coverage so the audience can join with their phones) - the performance is better suitable for a smaller room, since the only sound sources are the mobile phones of the audience - ideally the audience is able to move around and place their phones in different locations during the performance"
            }
          ]
        }
      ]
    }
  ]
}