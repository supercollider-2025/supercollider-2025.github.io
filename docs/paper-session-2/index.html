<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Paper Session 2 | The Future of SuperCollider</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Thursday, March 13, 2025
Room 426/428
4pm - 5:20pm
SCKinect
Evan Murray
4pm
go to paper
SCKinect is a SuperCollider plugin which allows users to interact with the Kinect v2 sensor in the server and language. Its core implementation contains a UGen called “Kinect,” designed to output human skeleton joint-tracking data to control buses. It will also include primitives designed to facilitate user interaction with Kinect devices. One example would be for users to be able to post all the available Kinect devices to the post window in SuperCollider, similar to how other human input devices work.">
    <meta name="generator" content="Hugo 0.142.0">
    
    
    
      <meta name="robots" content="index, follow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >



    

    
      

    

    

    
      <link rel="canonical" href="https://supercollider-2025.github.io/paper-session-2/">
    

    <meta property="og:url" content="https://supercollider-2025.github.io/paper-session-2/">
  <meta property="og:site_name" content="The Future of SuperCollider">
  <meta property="og:title" content="Paper Session 2">
  <meta property="og:description" content="Thursday, March 13, 2025
Room 426/428
4pm - 5:20pm
SCKinect Evan Murray
4pm
go to paper
SCKinect is a SuperCollider plugin which allows users to interact with the Kinect v2 sensor in the server and language. Its core implementation contains a UGen called “Kinect,” designed to output human skeleton joint-tracking data to control buses. It will also include primitives designed to facilitate user interaction with Kinect devices. One example would be for users to be able to post all the available Kinect devices to the post window in SuperCollider, similar to how other human input devices work.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:published_time" content="2025-01-24T21:30:10-05:00">
    <meta property="article:modified_time" content="2025-01-24T21:30:10-05:00">

  <meta itemprop="name" content="Paper Session 2">
  <meta itemprop="description" content="Thursday, March 13, 2025
Room 426/428
4pm - 5:20pm
SCKinect Evan Murray
4pm
go to paper
SCKinect is a SuperCollider plugin which allows users to interact with the Kinect v2 sensor in the server and language. Its core implementation contains a UGen called “Kinect,” designed to output human skeleton joint-tracking data to control buses. It will also include primitives designed to facilitate user interaction with Kinect devices. One example would be for users to be able to post all the available Kinect devices to the post window in SuperCollider, similar to how other human input devices work.">
  <meta itemprop="datePublished" content="2025-01-24T21:30:10-05:00">
  <meta itemprop="dateModified" content="2025-01-24T21:30:10-05:00">
  <meta itemprop="wordCount" content="987">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Paper Session 2">
  <meta name="twitter:description" content="Thursday, March 13, 2025
Room 426/428
4pm - 5:20pm
SCKinect Evan Murray
4pm
go to paper
SCKinect is a SuperCollider plugin which allows users to interact with the Kinect v2 sensor in the server and language. Its core implementation contains a UGen called “Kinect,” designed to output human skeleton joint-tracking data to control buses. It will also include primitives designed to facilitate user interaction with Kinect devices. One example would be for users to be able to post all the available Kinect devices to the post window in SuperCollider, similar to how other human input devices work.">

      
    
	
  </head><body class="ma0 avenir bg-near-white production">

    

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw5 hover-white no-underline white-90 dib flex items-center">
      
        <img src="/img/sc_cube_48x48_25.png" class="w100 mw5-ns mr2" alt="The Future of SuperCollider" />
        <span class="bg-black-50 white pa2 br2 dib">SuperCollider Symposium 2025</span>
      
    </a>    
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw5 dib pr3">
            <a class="hover-white no-underline white-90" href="/keynote-speakers/" title="Keynote Speakers page">
              <span class="bg-black-50 white pa2 br2 dib">
                Keynote Speakers
              </span>
            </a>
          </li>
          
          <li class="list f5 f4-ns fw5 dib pr3">
            <a class="hover-white no-underline white-90" href="/schedule/" title="Schedule page">
              <span class="bg-black-50 white pa2 br2 dib">
                Schedule
              </span>
            </a>
          </li>
          
          <li class="list f5 f4-ns fw5 dib pr3">
            <a class="hover-white no-underline white-90" href="/location/" title="Location page">
              <span class="bg-black-50 white pa2 br2 dib">
                Location
              </span>
            </a>
          </li>
          
          <li class="list f5 f4-ns fw5 dib pr3">
            <a class="hover-white no-underline white-90" href="/installations/" title="Installations page">
              <span class="bg-black-50 white pa2 br2 dib">
                Installations
              </span>
            </a>
          </li>
          
          <li class="list f5 f4-ns fw5 dib pr3">
            <a class="hover-white no-underline white-90" href="/documentation/" title="Documentation page">
              <span class="bg-black-50 white pa2 br2 dib">
                Documentation
              </span>
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>


    <main class="pb7" role="main">
      
  <div class="flex-l mt2 mw8 center">
    <article class="center cf pv5 ph3 ph4-ns mw7">
      <header>
        <h1 class="f1">
          Paper Session 2
        </h1>
      </header>
      <div class="nested-copy-line-height lh-copy f4 nested-links mid-gray">
        <p>Thursday, March 13, 2025<br>
<strong>Room 426/428</strong><br>
4pm - 5:20pm</p>
<h2 id="sckinect">SCKinect</h2>
<p><a href="/bios/#evan-murray">Evan Murray</a><br>
4pm</p>
<p><a href="/papers/Murray.pdf">go to paper</a></p>
<p>SCKinect is a SuperCollider plugin which allows users to interact with the Kinect v2 sensor in the server and language. Its core implementation contains a UGen called “Kinect,” designed to output human skeleton joint-tracking data to control buses. It will also include primitives designed to facilitate user interaction with Kinect devices. One example would be for users to be able to post all the available Kinect devices to the post window in SuperCollider, similar to how other human input devices work.</p>
<p>The shared benefit of the Kinect and SuperCollider is their ability to facilitate natural user interactions and artistic expression. The interpreted nature of SuperCollider and the server-language duality allows users to more-naturally communicate what they want to do, enabling them to focus on artistic expression. With the addition of the Kinect UGen, performers can interact with SuperCollider directly through their movement in an agent-less manner.</p>
<p>This project relates to interactive music systems and plugins for scsynth. Additionally, the GPU and CPU are both used here for visual and audio processing respectively. Although the GPU doesn’t directly provide audio, it is somewhat related to heterogeneous computing, as the CPU and GPU are both running the scsynth.</p>
<h2 id="sound-matching">Sound Matching</h2>
<p><a href="/bios/#gerard-roma">Gerard Roma</a><br>
4:20pm</p>
<p><a href="/papers/Roma.pdf">go to paper</a></p>
<p>Sound matching is a classic problem in sound synthesis, originally posed for controlling FM synthesis. Given a target sound, the problem is to find the parameters that a synthesizer could use to synthesize the target. It is common to distinguish between in-domain targets (sounds originally generated by the same synthesizer) and out-of-domain targets (sounds coming from other sources). In recent work, I explored using SuperCollider to create synthesizer ensembles which were paired with ensembles neural network regressors to match out-of-domain sounds.</p>
<p>This paper will explore a more practical use case, using the Fluid Corpus Manipulation toolbox  to incorporate sound matching into purely SC-based creative workflows. Given a synthesizer designed to produce a range of sounds, an out-of-domain sound can be used to predict a set of parameters. It is up to the user to design the synthesizer and target for more predictable or more serendipitous results. The paper will consider the case for real-time matching as well as for offloading some of the parameters, such as pitch and amplitude,  from the regression problem.</p>
<h2 id="a-case-study-of-music-glyph-notation-in-supercollider-using-smufl-fonts">A Case Study of Music Glyph Notation in SuperCollider using SMuFL fonts</h2>
<p><a href="/bios/#tom-hall">Tom Hall</a><br>
4:40pm</p>
<p><em>Keywords: extensions of sclang, live coding, multimedia music systems.</em></p>
<p>This paper describes a dynamic ‘live’ music notation project using common practice Western notation (CPWN) ‘glyphs’ natively within SuperCollider. The project builds on work presented in a new co-authored ‘Notations’ chapter in the forthcoming 2nd edition of ‘The SuperCollider Book’, and aims to avoid any interaction with third-party music software common to related projects. A barrier to native SuperCollider implementation begins with displaying music glyphs. Since a specialist music font is required, glyphs cannot usefully be pasted as Unicode UTF-8 into an IDE. The ‘MITHUnicode’ dependency helper class translates between Unicode binary and hex encodings so that non-ASCII characters can be easily referred to and displayed in SuperCollider. Using Unicode identifiers and working specifically with music fonts, the W3C initiative ‘SMuFL’ (Standard Music Font Layout) enables font interoperability between different notation softwares. Glyphs comprising an extended CPWN superset are given unique Unicode code points as well as metadata to assist with complex positioning considerations for music display.</p>
<p>Combining these approaches, the class ‘THNoteViewer’ enables the dynamic presentation of CPWN notes and complex chords within a SuperCollider Window in a format similar to Max software’s ‘nslider’ Object. To do this requires determining the correct placement of music noteheads and music glyphs such as ‘accidentals’ (flats, sharps etc). To avoid collisions between glyphs, SMuFL metadata is imported into the class, and dependency classes using computational geometry such as the ‘Graham scan’ algorithm and the Separating Axis Theorem are used to avoid glyph collision. Using a MVC design, THNoteViewer employs these approaches to display CPWN musical glyphs natively within SuperCollider in a manner that is both dynamic and responsive, whilst obeying complex engraving rules overlooked in some other related software environments.</p>
<h2 id="an-exploratory-analysis-of-sctweets-classification-and-similarity">An Exploratory Analysis of SCTweets Classification and Similarity</h2>
<p><a href="/bios/#fellipe-m.-martins">Fellipe M. Martins</a><br>
5pm</p>
<p>In the field of electroacoustic music, few artists and composers have published detailed information about their processes, and an even smaller number have provided a comprehensive blueprint, code, or instruction set that allows for a facsimile reconstruction of their work. With the advent of digital audio and the rise of creators making music exclusively on personal computers, it has become possible to track numerous details of the composition process when the software files are available. The use of programming languages for music composition has opened up a unique avenue for analysis, enabling musicologists to evaluate more clearly the relationship between sound results, high-level extracted features, and the tools and techniques used in their creation. An exceptional case is the SCTweets — 140-character SuperCollider code snippets that typically comprise compact sound procedures or dense compositions, shared on the microblogging platform Twitter (now X). We discuss the initial results of a timbral exploratory analysis on a specific set of SCTweets: tweets shared by Fredrik Olofsson on the repository sccode.org. Using the FluCoMa toolkit, we evaluate how timbral similarity might correlate with auditory criteria proposed by Pierre Schaeffer&rsquo;s theory. Additionally, we reveal various challenges and insights when correlating the usage of UGens and the provided timbral map. We selected Olofsson&rsquo;s SCTweets due to their unique use of the SuperCollider language, which often features codes utilizing only one number (666, 42, 1, 7), one type of UGen (SinOsc, Saw, LFCub), and codes that deliberately push DSP procedures to their limits to create new sounds, while adhering to the restriction of using only vanilla SC. We conclude the text with a discussion about the novelty of this analytical technique, presenting our findings and potential pitfalls.</p>

      </div>
    </article>
  </div>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://supercollider-2025.github.io/" >
    &copy;  The Future of SuperCollider 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
